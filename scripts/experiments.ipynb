{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc02a3ed",
   "metadata": {},
   "source": [
    "Testing with FINQA dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af73a07d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-08-14 11:51:17--  https://raw.githubusercontent.com/czyssrs/FinQA/main/dataset/train.json\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 78216616 (75M) [text/plain]\n",
      "Saving to: ‘train.json’\n",
      "\n",
      "train.json          100%[===================>]  74.59M  22.1MB/s    in 3.4s    \n",
      "\n",
      "2025-08-14 11:51:26 (21.9 MB/s) - ‘train.json’ saved [78216616/78216616]\n",
      "\n",
      "--2025-08-14 11:51:26--  https://raw.githubusercontent.com/czyssrs/FinQA/main/dataset/dev.json\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 10954658 (10M) [text/plain]\n",
      "Saving to: ‘dev.json’\n",
      "\n",
      "dev.json            100%[===================>]  10.45M  21.6MB/s    in 0.5s    \n",
      "\n",
      "2025-08-14 11:51:27 (21.6 MB/s) - ‘dev.json’ saved [10954658/10954658]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ivych/lora-finetuning-finQA/llama-env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded a subset of size 500 into a DataFrame.\n",
      "Successfully formatted and split the dataset. Training on 450 examples.\n",
      "### Question:\n",
      "what was the total amount of unfunded commitments in millions as of the end of 2008 and 2007?\n",
      "\n",
      "### Answer:\n",
      "290.0\n"
     ]
    }
   ],
   "source": [
    "# 2. Download the raw data files from the official GitHub repository\n",
    "!wget https://raw.githubusercontent.com/czyssrs/FinQA/main/dataset/train.json\n",
    "!wget https://raw.githubusercontent.com/czyssrs/FinQA/main/dataset/dev.json\n",
    "\n",
    "# 3. Load the JSON files into a Pandas DataFrame\n",
    "import pandas as pd\n",
    "import json\n",
    "from datasets import Dataset\n",
    "\n",
    "try:\n",
    "    with open('train.json', 'r') as f:\n",
    "        train_data = json.load(f)\n",
    "\n",
    "    # Load the data into a DataFrame\n",
    "    train_df = pd.DataFrame(train_data)\n",
    "\n",
    "    # Optional: Take a subset of the data for your minimal experiment\n",
    "    subset_df = train_df.head(500)\n",
    "\n",
    "    print(f\"Successfully loaded a subset of size {len(subset_df)} into a DataFrame.\")\n",
    "\n",
    "    # 4. Preprocess and Format the Data\n",
    "    # The keys in the dataframe are: id, pre_text, post_text, table, qa\n",
    "    # The question is in row['qa']['question']\n",
    "    # The answer is in row['qa']['exe_ans']\n",
    "\n",
    "    formatted_data = []\n",
    "    for _, row in subset_df.iterrows():\n",
    "        question = row['qa']['question']\n",
    "        answer = row['qa']['exe_ans']\n",
    "\n",
    "        # Format the data into the instruction prompt template\n",
    "        text = f\"### Question:\\n{question}\\n\\n### Answer:\\n{answer}\"\n",
    "        formatted_data.append({\"text\": text})\n",
    "\n",
    "    # 5. Convert the Pandas DataFrame to a Hugging Face Dataset\n",
    "    formatted_df = pd.DataFrame(formatted_data)\n",
    "    finqa_dataset = Dataset.from_pandas(formatted_df)\n",
    "\n",
    "    # 6. Split the dataset into a small train and test set\n",
    "    finqa_dataset_splits = finqa_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "    train_dataset = finqa_dataset_splits['train']\n",
    "    eval_dataset = finqa_dataset_splits['test']\n",
    "\n",
    "    print(f\"Successfully formatted and split the dataset. Training on {len(train_dataset)} examples.\")\n",
    "    print(train_dataset[0]['text'])\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116ed4a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ivych/lora-finetuning-finQA/llama-env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"meta-llama/Llama-2-7b-hf\"  # Or Llama-3-8b if you have access\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model_qlora = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    load_in_4bit=True,  # For QLoRA (set to False for standard LoRA/DoRA)\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"auto\"\n",
    ")\n",
    "\n",
    "model_qlora.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "print(\"Model and tokenizer loaded successfully.\")\n",
    "\n",
    "#model_ = AutoModelForCausalLM.from_pretrained(\n",
    " #   model_name,\n",
    "  #  load_in_4bit=False,  # For QLoRA (set to False for standard LoRA/DoRA)\n",
    "   # device_map=\"auto\"\n",
    "#)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32b3d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tokenize_fn(example):\n",
    "    return tokenizer(\n",
    "        example[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=256\n",
    "        #return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "tokenized_train = train_dataset.map(tokenize_fn, batched=True, remove_columns=[\"text\"])\n",
    "tokenized_eval = eval_dataset.map(tokenize_fn, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "print(\"Tokenization complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b4053c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\"\"\"lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=16,  # Rank\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"]  # Most common for LLaMA\n",
    ")\n",
    "\n",
    "model = get_peft_model(model_, lora_config)\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739834a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"from peft import DORAConfig\n",
    "\n",
    "dora_config = DORAConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"]\n",
    ")\n",
    "\n",
    "model = get_peft_model(model_, dora_config)\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a8a75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "qlora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=4,  # Rank\n",
    "    lora_alpha=8,\n",
    "    lora_dropout=0.05,\n",
    "    max_length = 256,\n",
    "    target_modules=[\"q_proj\"]  # Most common for LLaMA, \"v_proj\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model_qlora, qlora_config)\n",
    "print(\"PEFT model created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84139d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./llama-peft-finetuned\",\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=50,\n",
    "    save_steps=500,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    remove_unused_columns=False,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_eval\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "print(\"Training complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce29664",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./llama_peft_weights\")\n",
    "tokenizer.save_pretrained(\"./llama_peft_weights\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad846091",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import merge_and_unload\n",
    "merged_model = merge_and_unload(model)\n",
    "merged_model.save_pretrained(\"./llama_finetuned_full\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
